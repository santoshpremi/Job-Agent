// Step 0: LLM Integration and Smart Fallback System
// This demonstrates how the agent system integrates with multiple LLM providers
import { generateText, getProviderStatus } from '../utils/ai.js';

async function main() {
  console.log("\n" + "=".repeat(60));
  console.log("ğŸš€ STEP 0: LLM Integration and Smart Fallback System");
  console.log("=".repeat(60));
  
  console.log("\nğŸ“‹ This step demonstrates:");
  console.log("1. How multiple LLM providers are integrated");
  console.log("2. Smart fallback system (OpenRouter â†’ OpenAI â†’ Groq)");
  console.log("3. Provider availability caching");
  console.log("4. Automatic model selection for best performance");
  
  // Check provider status
  console.log("\nğŸ” Checking LLM Provider Status:");
  const status = getProviderStatus();
  console.log("OpenRouter:", status.openrouter ? "âœ… Available" : "âŒ Unavailable");
  console.log("OpenAI:", status.openai ? "âœ… Available" : "âŒ Unavailable");
  console.log("Groq:", status.groq ? "âœ… Available" : "âŒ Unavailable");
  console.log("Best Available:", status.bestAvailable || "None");
  
  // Test text generation with smart fallback
  console.log("\nğŸ§ª Testing Text Generation with Smart Fallback:");
  const testPrompt = "Explain in one sentence how AI agents work.";
  
  try {
    console.log(`\nğŸ“ Prompt: "${testPrompt}"`);
    const response = await generateText(testPrompt);
    console.log(`ğŸ¤– Response: ${response}`);
    console.log("âœ… Text generation successful!");
  } catch (error) {
    console.log(`âŒ Text generation failed: ${error.message}`);
  }
  
  console.log("\n" + "=".repeat(60));
  console.log("ğŸ¯ Key Learning Points:");
  console.log("â€¢ The system automatically selects the best available LLM provider");
  console.log("â€¢ Failed providers are marked and skipped in future calls");
  console.log("â€¢ No repeated availability checks - efficient and cost-effective");
  console.log("â€¢ Easy to add new providers to the fallback chain");
  console.log("=".repeat(60));
}

main();